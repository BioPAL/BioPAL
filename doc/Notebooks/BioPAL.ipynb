{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "extraordinary-creator",
   "metadata": {},
   "source": [
    "BioPAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-costs",
   "metadata": {},
   "source": [
    "# BioPAL First AGB Tutorial\n",
    "\n",
    "The BIOMASS Product Algorithm Laboratory (BioPAL) hosts official tools for processing and analysing ESA\\'s BIOMASS mission data.\n",
    "\n",
    "-   Website: www.biopal.org\n",
    "-   Mailing: <biopal@esa.int>\n",
    "\n",
    "_Disclamer:\n",
    "this tutorial is released under MIT license, it is experimental and may change without notice._\n",
    "\n",
    "\n",
    "## Tutorial Objective\n",
    "\n",
    "In this tutorial, following topics will be discussed:\n",
    "- requirements, biopal installation and environment preparation for this notebook to work\n",
    "- setup and run of the Above Ground Biomass (AGB) processor (interactive notebook section)\n",
    "\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- The installation procedure described here makes use of the open-source package management system [conda](https://docs.conda.io/projects/conda/en/latest/).\n",
    "- Python >=3.6\n",
    "\n",
    "\n",
    "## Installation and Jupyter Notebook Execution\n",
    "\n",
    "\n",
    "1.  Download and unzip the current [BioPAL distribution](https://github.com/BioPAL/BioPAL) to your local hard drive.\n",
    "\n",
    "\n",
    "2.  In a *conda* command window, enter in the directory where the BioPAL has been unzipped, and type the following instruction, which creates a biopal environment ( `environment.yml` is present into the main folder of BioPAL distribution ):\n",
    "\t\n",
    "        conda env create --file environment.yml\n",
    "\n",
    "3.  In the same *conda* command window, type the following instruction, which activates the created biopal environment:\n",
    "\t\n",
    "        conda activate biopal\n",
    "\n",
    "4.  In the same *conda* command window, install Jupyter notebook \n",
    "\t\n",
    "        conda install -c conda-forge jupyterlab\n",
    "\n",
    "5.  In the same *conda* command window, open this Jupyter Notebook typing the following instructions:\n",
    "\n",
    "        cd doc/Notebooks\n",
    "        jupyter notebook\n",
    "        \n",
    "\n",
    "# Setup AGB Processor\n",
    "\n",
    "\n",
    "## Main Ingredients\n",
    "\n",
    "Main ingredients needed to run this tutorial:\n",
    "\n",
    "-   `BioPAL/biopal/conf/Configuration_File.xml`: is the BioPAL configuration file (this tutorial is focused on the AGB only)\n",
    "\n",
    "-   `BioPAL/inputs/Input_File.xml`: input file, to be set by the user before running an instance of the processing\n",
    "\n",
    "This Notebook is supposed to be executed from `BioPAL/biopal/doc/Notebooks/BioPAL.ipynb`.\n",
    "\n",
    "Setup of the AGB processor includes:\n",
    "-   Input File check / preparation\n",
    "-   Configuration File check / preparation\n",
    "\n",
    "\n",
    "## BioPAL datasets\n",
    "\n",
    "BioPAL gives easy access to several datasets that are used for examples in the documentation and testing. These datasets are hosted on the MAAP and must be downloaded for use. Contact <biopal@esa.int> to receive access to the dataset and for more information.\n",
    "Each dataset is composed by two folders:\n",
    "-   `dataSet`: folder containing L1 stacks of coregistered and calibrated SLC (Single Look Complex) data\n",
    "-   `auxiliary_data_pf`: folder containing auxiliary products which are related to each particular dataSet \n",
    "\n",
    "\n",
    "## Prepare Input\n",
    "\n",
    "Open the `inputs/Input_File.xml` and focus on the following sections:\n",
    "\n",
    "-   `L2_product`: specifies the algorithm to be run, here kept to `AGB` to enable the Above Ground Biomass processing (objective of this tutorial)\n",
    "-   `output_specification` section: \n",
    "   * `output_folder`: full path of the folder where processing will save the products. Each run will generate a new sub-folder tagged with the processing time stamp (only in case of single run of the complete suite, see later section)\n",
    "   * `geographic_grid_sampling`:value, in meters, of the sampling for the final processed product on geographic map, valid (east and north directions)\n",
    "-   `dataset_query` section:\n",
    "   * `L1C_repository`: should be put equal to the `dataSet` folder described above\n",
    "   * `L1C_date`start and stop values to select L1 products, in UTC format\n",
    "   * `auxiliary_products_folder`: should be put equal to the `auxiliary_data_pf` folder described above; in particular, the sub-folder ReferenceAGB, contains the reference data used to calibrate the AGB estimation inversion algorithm\n",
    "   * `geographic_boundaries_polygon`: a set of 3 or more latitude-longitude points describing a polygon used to select L1 products\n",
    "\n",
    "Note: the Input_File.xml in the repository is already filled with example values.\n",
    "\n",
    "\n",
    "## Prepare Configuration\n",
    "\n",
    "The BioPAL configuration file is `biopal/conf/Configuration_File.xml` and it is already filled with default AGB parameters to get valid estimation results; main parameters the users can modify are described hereafter, for more details refer to the complete documentation.\n",
    "\n",
    "-   `ground_cancellation` section: \n",
    "   * `enhanced_forest_height`: forest height in meters used in the processing to determine the vertical wavenumber for which the ground cancelled data is generated (only with more than two acquisitions)  \n",
    "-   `estimate_agb` section: \n",
    "   * `number_of_tests`: number of tests executed for AGB estimation following a random sampling scheme of estimation and calibration points (reference)\n",
    "   * `fraction_of_roi_per_test`: percentage of estimation points randomly selected for each test   \n",
    "   * `fraction_of_cal_per_test`: percentage of calibration points randomly selected for each test\n",
    "   * `intermediate_ground_averaging`: ground cancelled averaging window size in meters on ground: it should be less or equal to product_resolution/2 (see below)\n",
    "   * `product_resolution`: output product resolution (AGB) in meters on geographic map   \n",
    "   * `distance_sampling_area`: estimation points distance in meters on geographic map\n",
    "   * `estimation_valid_values_limits`: validity range of the output product in t/ha, invalid values will be masked out\n",
    "   \n",
    "\n",
    "# Run AGB Processor\n",
    "\n",
    "There are two ways to run the AGB processor:\n",
    "-   execution of the \"main\" in a single run\n",
    "-   manual execution of each specific AGB step\n",
    "\n",
    "In both cases, the first step is to add the `biopal` folder to the Python path.\n",
    "In the following code, it is supposed to have this notebook in `BioPAL/biopal/doc/Notebooks/BioPAL.ipynb`\n",
    "If this is not the case, you can manually set the `biopal_path` in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "accessory-laundry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BioPAL path \"C:\\ARESYS_PROJ\\BioPAL\" has been succesfully added to the python path\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # warnings silenced for convenience\n",
    "\n",
    "# get the biopal folder path, supposing that we are in the working directory \"biopal/doc/Notebooks/\"\n",
    "notebook_working_dir = Path.cwd()\n",
    "biopal_path = notebook_working_dir.parent.parent\n",
    "sys.path.append( str(biopal_path) )\n",
    "\n",
    "# Try an import to verify the python path\n",
    "try:\n",
    "    from biopal.__main__ import biomassL2_processor_run\n",
    "    \n",
    "    print('The BioPAL path \"{}\" has been succesfully added to the python path'.format(biopal_path))\n",
    "\n",
    "except Exception:\n",
    "    raise Exception( 'The BioPAL Notebook needs to be executed from the folder \"BioPAL/doc/Notebooks\" and the \"biopal\" environment should be enabled too. It has been executed from \"{}\" instead.'.format(notebook_working_dir) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-poster",
   "metadata": {},
   "source": [
    "# Run AGB Processor by execution of each AGB step\n",
    "\n",
    "The AGB processor can be executed by manual run of each AGB step (APPs): each APP computes the inputs needed by the following one, and updates the input file adding each APP specific section, and (when needed) also the configuration file.\n",
    "\n",
    "### dataset_query APP run\n",
    "\n",
    "The first APP to be executed, the `dataset_query`, is in charge of getting from the `L1C_repository` only the stacks matching the specified temporal and geographical region of interest. \n",
    "\n",
    "It takes as input the `inputs\\Input_File.xml` path, prepared in the above steps of this tutorial.\n",
    "\n",
    "The input file will be updated adding the new section `stack_based_processing` needed to the following APP.\n",
    "The updated input file will be saved into the output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "smooth-quantity",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query started...\n",
      "Query completed.\n",
      "The Input_File has been updated with the new section \"stack_based_processing\" and saved to C:\\bio\\outNew\\AGB\\InputFile_StackBasedProcessingAGB.xml\n"
     ]
    }
   ],
   "source": [
    "input_path = str( Path.home().joinpath( biopal_path, \"inputs\", \"Input_File.xml\") ) \n",
    "from biopal.dataset_query.dataset_query import dataset_query\n",
    "\n",
    "# Initialize dataset query APP (no configuration file needed in this case)\n",
    "dataset_query_obj = dataset_query()\n",
    "\n",
    "# Run dataset query APP\n",
    "print('Query started...')\n",
    "input_path_from_query, _, _  = dataset_query_obj.run( input_path )\n",
    "\n",
    "print('The Input_File has been updated with the new section \"stack_based_processing\" and saved to {}'.format(input_path_from_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-princess",
   "metadata": {},
   "source": [
    "Hereafter we check the new `stack_based_processing` section added to the input file: as an example we print all the SLC SAR images acquisition IDs selected by the query and the path of the DTM projected in radar coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "literary-karen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLC SAR images (slant range geometry) stacks and acquisitions found from the query\n",
      "\n",
      "    Stack:\n",
      "     GC_02_H_230.00_RGSW_00_RGSBSW_00_AZSW_00\n",
      "        Acquisitions:\n",
      "         GC_02_H_230.00_RGSW_00_RGSBSW_00_AZSW_00_BSL_00\n",
      "         GC_02_H_230.00_RGSW_00_RGSBSW_00_AZSW_00_BSL_01\n",
      "         GC_02_H_230.00_RGSW_00_RGSBSW_00_AZSW_00_BSL_02\n",
      "\n",
      "    Stack:\n",
      "     GC_02_H_275.00_RGSW_00_RGSBSW_00_AZSW_00\n",
      "        Acquisitions:\n",
      "         GC_02_H_275.00_RGSW_00_RGSBSW_00_AZSW_00_BSL_00\n",
      "         GC_02_H_275.00_RGSW_00_RGSBSW_00_AZSW_00_BSL_01\n",
      "         GC_02_H_275.00_RGSW_00_RGSBSW_00_AZSW_00_BSL_02\n",
      "\n",
      " The above stacks with the acquisitions can be found at following path:\n",
      " C:\\bio\\demo_lope_two\\dataSet\n",
      "\n",
      " Full path of the reference height file valid for the above stack GC_02_H_275.00_RGSW_00_RGSBSW_00_AZSW_00: \n",
      " C:\\bio\\demo_lope_two\\auxiliary_data_pf\\Geometry\\ReferenceHeight\\GC_02_H_275.00_RGSW_00_RGSBSW_00_AZSW_00\n"
     ]
    }
   ],
   "source": [
    "    from biopal.io.xml_io import parse_input_file\n",
    "\n",
    "    # read the updated input file:\n",
    "    input_params_obj = parse_input_file(input_path_from_query)\n",
    "    \n",
    "    # get the new section stack_based_processing:\n",
    "    stack_based_processing_obj = input_params_obj.stack_based_processing\n",
    "    \n",
    "    # read the IDs of the stacks found from the query:\n",
    "    found_stack_ids = stack_based_processing_obj.stack_composition.keys()\n",
    "    print('SLC SAR images (slant range geometry) stacks and acquisitions found from the query')\n",
    "    for stack_id, acquisition_ids in stack_based_processing_obj.stack_composition.items():\n",
    "        print('\\n    Stack:')\n",
    "        print('    ', stack_id)\n",
    "        print('        Acquisitions:')\n",
    "        for acq_id in acquisition_ids:\n",
    "            print('        ', acq_id)\n",
    "    dataSet_path = input_params_obj.dataset_query.L1C_repository\n",
    "    print('\\n The above stacks with the acquisitions can be found at following path:\\n {}'.format( dataSet_path ))\n",
    "    \n",
    "    # Print the path of the DTM projected in radar coordinates\n",
    "    reference_height_path = Path.home().joinpath(\n",
    "        dataSet_path,\n",
    "        stack_based_processing_obj.reference_height_file_names[stack_id]\n",
    "        )\n",
    "    print('\\n Full path of the reference height file valid for the above stack {}: \\n {}'.format(stack_id, reference_height_path))                                                            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-celebrity",
   "metadata": {},
   "source": [
    "*Stefanie*: with matplotlib/plotly plot here one of the SLCs and the reference height printed above:\n",
    "-   10*log10( abs(SLC)^2 ) (rows: slant range, columns: azimuth, use pixel spacing for representing in meters rather than pixel index)\n",
    "-   reference height in meters (rows: slant range, columns: azimuth, use pixel spacing for representing in meters rather than pixel index)\n",
    "\n",
    "Notes: \n",
    "Read pixel spacings from one of the dataSet data i.e. /demo_lope_two\\dataSet\\GC_02_H_230.00_RGSW_00_RGSBSW_00_AZSW_00_BSL_00\\GC_02_H_230.00_RGSW_00_RGSBSW_00_AZSW_00_BSL_00_0001.xml\n",
    "<LinesStep unit=\"m\">3.63676</LinesStep> is azimuth pixel spacing\n",
    "<SamplesStep unit=\"m\">11.988876</SamplesStep> is slant range pixel spacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-earthquake",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painful-variance",
   "metadata": {},
   "source": [
    "### StackBasedProcessingAGB APP run\n",
    "\n",
    "The second APP to be executed, is the `StackBasedProcessingAGB`, which takes the above stacks and preprocesses them with ground cancellation and geocoding.\n",
    "\n",
    "It takes as input:\n",
    "-   the Input File path, the one prepared by the `dataset_query` APP, containing the new `stack_based_processing` section\n",
    "-   the Configuration File path\n",
    "\n",
    "The input file will be updated in the end adding the new section `core_processing_agb`for the following APP.\n",
    "The configuration file is updated too.\n",
    "The updated input file and configuration file will be saved into output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "heavy-gazette",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGB stack-based processing APP started...\n",
      "AGB stack-based processing APP ended correctly.\n",
      "\n",
      "The Input_File has been updated with the new section \"CoreProcessingAGB\" and saved to C:\\bio\\outNew\\AGB\\Input_File_CoreProcessingAGB.xml\n"
     ]
    }
   ],
   "source": [
    "from biopal.agb.main_AGB import StackBasedProcessingAGB\n",
    "configuration_file_path = str( Path.home().joinpath( biopal_path, \"biopal\", \"conf\", \"Configuration_File.xml\" ) )\n",
    "\n",
    "# Initialize Stack Based Processing AGB APP\n",
    "stack_based_processing_obj = StackBasedProcessingAGB( configuration_file_path )\n",
    "\n",
    "# Run Stack Based Processing AGB APP\n",
    "print('AGB stack-based processing APP started...')\n",
    "input_file_from_stack_based, configuration_file_updated = stack_based_processing_obj.run( input_path_from_query )\n",
    "    \n",
    "    \n",
    "print('The Input_File has been updated with the new section \"core_processing_agb\" and saved to {}'.format(input_file_from_stack_based))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-silver",
   "metadata": {},
   "source": [
    "Hereafter we print the paths of some of the computed data, all stored in the output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "damaged-municipality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Path of ground cancelled data in slant range geometry: \n",
      " C:\\bio\\outNew\\AGB\\AGB\\Products\\breakpoints\\ground_cancelled_SR_GC_02_H_230.00_RGSW_00_RGSBSW_00_AZSW_00.npy\n",
      "\n",
      "  Path of ground cancelled HH data, geocoded: \n",
      " C:\\bio\\outNew\\AGB\\AGB\\Products\\temp\\geocoded\\GC_02_H_230.00_RGSW_00_RGSBSW_00_AZSW_00\\sigma0_hh.tif\n",
      "\n",
      "  Path of incidence angle map, geocoded: \n",
      " C:\\bio\\outNew\\AGB\\AGB\\Products\\temp\\geocoded\\GC_02_H_230.00_RGSW_00_RGSBSW_00_AZSW_00\\theta.tif\n"
     ]
    }
   ],
   "source": [
    "# Some of the computed optuput paths:\n",
    "input_params_obj = parse_input_file(input_file_from_stack_based)\n",
    "output_folder = input_params_obj.output_specification.output_folder\n",
    "\n",
    "ground_canc_sr_path = Path.home().joinpath( \n",
    "    output_folder, \n",
    "    'Products', 'breakpoints', 'ground_cancelled_SR_GC_02_H_230.00_RGSW_00_RGSBSW_00_AZSW_00.npy')\n",
    "print('\\n Path of ground cancelled data in slant range geometry: \\n {}'.format(ground_canc_sr_path))\n",
    "\n",
    "ground_canc_gr_path = Path.home().joinpath( \n",
    "    output_folder, \n",
    "    'Products','temp','geocoded','GC_02_H_230.00_RGSW_00_RGSBSW_00_AZSW_00','sigma0_hh.tif')\n",
    "print('\\n  Path of ground cancelled HH data, geocoded: \\n {}'.format(ground_canc_gr_path))\n",
    "\n",
    "theta_inc_path = Path.home().joinpath( \n",
    "    output_folder, \n",
    "    'Products','temp','geocoded','GC_02_H_230.00_RGSW_00_RGSBSW_00_AZSW_00','theta.tif')\n",
    "print('\\n  Path of incidence angle map, geocoded: \\n {}'.format(theta_inc_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accomplished-ensemble",
   "metadata": {},
   "source": [
    "*Stefanie*: plot here:\n",
    "-   ground_canc_sr_path: 10*log10( abs( GC_SR_HH )^2 )(SAR coordinates Range/Azimuth) (rows: slant range, columns: azimuth, use pixel spacing for representing in meters rather than pixel index)\n",
    "-   ground_canc_gr_path: 10*log10( abs( GC_GR_HH )^2 ) (rows: north, columns: east, use pixel spacing from geotif geotransform for representing in meters rather than pixel index)\n",
    "-   theta_inc_path: the incidence angle in degrees (rows: north, columns: east, use pixel spacing from geotif geotransform for representing in meters rather than pixel index)\n",
    "\n",
    "Notes: \n",
    "GC_SR_HH = numpy.load(ground_canc_sr_path, allow_pickle=True).item(0)['hh']\n",
    "\n",
    "Read pixel spacings from one of the dataSet data i.e. /demo_lope_two\\dataSet\\GC_02_H_230.00_RGSW_00_RGSBSW_00_AZSW_00_BSL_00\\GC_02_H_230.00_RGSW_00_RGSBSW_00_AZSW_00_BSL_00_0001.xml\n",
    "<LinesStep unit=\"m\">3.63676</LinesStep> is azimuth pixel spacing\n",
    "<SamplesStep unit=\"m\">11.988876</SamplesStep> is slant range pixel spacing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-tanzania",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corrected-sending",
   "metadata": {},
   "source": [
    "### CoreProcessingAGB APP run\n",
    "\n",
    "The third and last APP to be executed is `CoreProcessingAGB`, which is in charge of performing the AGB estimation.\n",
    "\n",
    "It takes as input:\n",
    "-   the Input File path, the one prepared by the `stack_based_processing` APP, containingh the new `core_processing_agb` section\n",
    "-   the Configuration File path, the one updated by the `stack_based_processing` APP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-boating",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGB core-processing APP started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:AGB: limit units for parameter agb_1_est_db and associated observable agb_1_cal_db do not match (db_t/ha and t/ha). However, this could still be OK: the observable source unit is t/ha and transform is db. Proceed with caution.\n",
      "WARNING:root:AGB: skipping formula terms: 1, 2, 3 due to lack of useful data for observables: agb_1_cal_db.\n"
     ]
    }
   ],
   "source": [
    "from biopal.agb.main_AGB import CoreProcessingAGB\n",
    "\n",
    "# Initialize Core Processing AGB APP\n",
    "agb_processing_obj = CoreProcessingAGB( configuration_file_updated )\n",
    "    \n",
    "# Run Main APP #2: AGB Core Processing\n",
    "print('AGB core-processing APP started...')\n",
    "agb_processing_obj.run(input_file_from_stack_based)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-landscape",
   "metadata": {},
   "source": [
    "Hereafter we print the paths of the final product and the calibration product used in the algorithm.\n",
    "Products are geocoded to [EQUI7 map](https://github.com/TUW-GEO/Equi7Grid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "legal-patrick",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Path of the final AGB estimation product, in EQUI7 geometry: \n",
      " C:\\bio\\outNew\\AGB\\AGB\\Products\\global_AGB\\AF050M\\E042N048T6\\agb_1_est_db_backtransf_.tif\n",
      "\n",
      " Path of the input calibration zone SAR image used: \n",
      " C:\\bio\\demo_lope_two\\auxiliary_data_pf\\ReferenceAGB\\cal_05_no_errors.tif\n"
     ]
    }
   ],
   "source": [
    "# Final products full paths:\n",
    "input_params_obj = parse_input_file(input_file_from_stack_based)\n",
    "output_folder = input_params_obj.output_specification.output_folder\n",
    "\n",
    "final_estimation_path = Path.home().joinpath( \n",
    "    output_folder, \n",
    "    'Products','global_AGB','AF050M','E042N048T6','agb_1_est_db_backtransf_.tif')\n",
    "print('\\n Path of the final AGB estimation product, in EQUI7 map geometry: \\n {}'.format(final_estimation_path))\n",
    "    \n",
    "reference_agb_folder = input_params_obj.stack_based_processing.reference_agb_folder\n",
    "calibration_path = Path.home().joinpath( \n",
    "    reference_agb_folder, 'cal_05_no_errors.tif')\n",
    "print('\\n Path of the input calibration data used: \\n {}'.format(calibration_path))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-gregory",
   "metadata": {},
   "source": [
    "*Stefanie* plot here:\n",
    "\n",
    "final_estimation_path: the final product, estimation of the AGB in t/ha, in Equi7 geometry (rows: north, columns: east, use pixel spacing from geotif geotransform for representing in meters rather than pixel index)\n",
    "\n",
    "For validation:\n",
    "\n",
    "-   full LIDAR AGB image which is on the FTP here /aux_for_agb_dev/lope_lidar/lidar_agb/EQUI7_AF050M/E045N048T3/lidar_AGB_AF050M_E045N048T3.tif\n",
    "\n",
    "   calibration_path: calibration area is a subset of the full LIDAR\n",
    "\n",
    "-    2D istogram:\n",
    "final_estimation_path vs LIDAR\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-sixth",
   "metadata": {},
   "source": [
    "## Run AGB Processor, complete \n",
    "\n",
    "The following code will run the complete AGB processor, by calling the `biomassL2_processor_run` function, which automatically call the APPs in sequence.\n",
    "\n",
    "It takes as input:\n",
    "-   the path of the `Input_File.xml`\n",
    "-   the folder containing the `Configuration_File.xml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-private",
   "metadata": {},
   "outputs": [],
   "source": [
    "from biopal.__main__ import biomassL2_processor_run\n",
    "input_path = str( Path.home().joinpath( biopal_path, \"inputs\", \"Input_File.xml\") ) \n",
    "configuration_folder = str( Path.home().joinpath(biopal_path, \"biopal\", \"conf\") )\n",
    "\n",
    "# run the AGB processor\n",
    "biomassL2_processor_run( input_path, configuration_folder )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
